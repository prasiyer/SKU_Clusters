{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "import hdbscan\n",
    "from sklearn.manifold import TSNE\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "# Main change from v2 to v3 is that the specific regions have been replaced with general regions\n",
    "cmd_data = pd.read_csv('/data2/home/prasannaiyer/Projects/SKU_Cluster_Local/SKU_Clusters/Data/cmd_attributes_v3_upload.csv',\\\n",
    "    encoding = 'latin-1', decimal = '.', thousands = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_data['p_bu'] = cmd_data['p_bu'].replace({'AW': 'AG', 'CW': 'CE'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_count = cmd_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_input = ['Attr1_Str_qu', 'Attr2_OpPr_qu','Attr3_Costamount_norm',\n",
    "       'Attr4_RdDi_norm', 'Attr5_BrDi_norm', 'Attr5_Str_Type1',\n",
    "       'Attr5_Str_Type2', 'Attr5_Str_Type3', 'Attr6_Major_Type1',\n",
    "       'Attr6_Major_Type2', 'Attr9_Frctn0', 'Attr9_Frctn1', 'Attr10_Snsr0', 'Attr10_Snsr1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_input_cost_model = ['Attr1_Str_qu', 'Attr2_OpPr_qu',\n",
    "       'Attr4_RdDi_norm', 'Attr5_BrDi_norm', 'Attr5_Str_Type1',\n",
    "       'Attr5_Str_Type2', 'Attr5_Str_Type3', 'Attr6_Major_Type1',\n",
    "       'Attr6_Major_Type2', 'Attr9_Frctn0', 'Attr9_Frctn1', 'Attr10_Snsr0', 'Attr10_Snsr1']\n",
    "attr_input_subset = ['Attr1_Str_qu', 'Attr2_OpPr_qu', 'Attr3_Costamount_norm',\n",
    "       'Attr4_RdDi_norm', 'Attr5_BrDi_norm', 'Attr9_Frctn0', 'Attr9_Frctn1']\n",
    "attr_input_subset_cost = ['Attr1_Str_qu', 'Attr2_OpPr_qu', 'Attr3_Costamount_norm',\n",
    "       'Attr4_RdDi_norm', 'Attr5_BrDi_norm', 'Attr9_Frctn0', 'Attr9_Frctn1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an autoencoder class. For init function, input would be the input data, output data, layers. Also in the init function, create the autoencoder model\n",
    "# For the fit function, input would be the number of epochs, batch size, and the learning rate\n",
    "\n",
    "class auto_encoder:\n",
    "    def __init__(self, input_data, output_data, ae_layers, alpha_ae = 0.2):\n",
    "        self.input_data = input_data\n",
    "        self.output_data = output_data\n",
    "        self.ae_layers = ae_layers\n",
    "        self.alpha_ae = alpha_ae\n",
    "        self.autoencoder = self.create_autoencoder()\n",
    "                \n",
    "    def create_autoencoder(self):\n",
    "        # create the input layer\n",
    "        encoder_input = Input(shape=(self.input_data.shape[1],))\n",
    "        nn_layer = encoder_input\n",
    "        # create the encoder layers\n",
    "        for i in range(len(self.ae_layers)):\n",
    "            layer_name = 'encoder_layer_' + str(i+1)\n",
    "            layer_name = Dense(self.ae_layers[i])(nn_layer)\n",
    "            layer_name = LeakyReLU(alpha=self.alpha_ae)(layer_name)\n",
    "            nn_layer = layer_name\n",
    "        encoder_output = nn_layer\n",
    "        self.encoder_model = Model(encoder_input, encoder_output)\n",
    "        # create the decoder layers\n",
    "        for i in range(len(self.ae_layers)-2, -1, -1):\n",
    "            layer_name = 'decoder_layer_' + str(i+1)\n",
    "            layer_name = Dense(self.ae_layers[i])(nn_layer)\n",
    "            layer_name = LeakyReLU(alpha=self.alpha_ae)(layer_name)\n",
    "            nn_layer = layer_name\n",
    "        decoder_output = nn_layer\n",
    "        # create the output layer\n",
    "        autoencoder_output = Dense(self.output_data.shape[1], activation = 'linear')(nn_layer)\n",
    "        self.ae_model = Model(encoder_input, autoencoder_output)\n",
    "        # compile the model\n",
    "        self.ae_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Create a function to fit the model\n",
    "    def autoencoder_fit(self, epochs = 500, batch_size = 32, verbose = 1):\n",
    "        self.ae_model_hist = self.ae_model.fit(self.input_data, self.output_data, epochs = epochs, batch_size = batch_size, verbose = verbose)\n",
    "        self.training_loss = self.ae_model_hist.history['loss'][-1]\n",
    "\n",
    "    # Create a function to predict the embeddings from the encoder model\n",
    "    def predict_embeddings(self, input_data):\n",
    "        self.embeddings = self.encoder_model.predict(input_data)\n",
    "\n",
    "    # Create a function to plot the training loss\n",
    "    def plot_training_loss(self):\n",
    "        plt.figure(figsize = (10, 6))\n",
    "        ax = sns.lineplot(x = range(1, len(self.ae_model_hist.history['loss']) + 1), y = self.ae_model_hist.history['loss'])\n",
    "        ax.set_xlabel('Epochs')\n",
    "        ax.set_ylabel('Training Loss')\n",
    "        ax.set_title('Training Loss vs Epochs')\n",
    "        ax.grid(linestyle='-', linewidth='0.5', color='red')\n",
    "\n",
    "    # create a function to print the loss for epochs at specified intervals\n",
    "    def print_loss(self, interval = 500):\n",
    "        for i in range(0, len(self.ae_model_hist.history['loss']), interval):\n",
    "            print(f'Loss at epoch {i}: {self.ae_model_hist.history[\"loss\"][i]:.4f}')\n",
    "        # print the loss at the last epoch using f-string\n",
    "        print(f'Loss at epoch {len(self.ae_model_hist.history[\"loss\"])}: {self.ae_model_hist.history[\"loss\"][-1]:.4f}')\n",
    "        \n",
    "        # print minimum loss and the epoch at which it occurs\n",
    "        # print(f'Minimum loss: {min(self.ae_model_hist.history['loss']):.4f} at epoch {self.ae_model_hist.history['loss'].index(min(self.ae_model_hist.history['loss']))}')\n",
    "        print(f'Minimum loss: {min(self.ae_model_hist.history[\"loss\"]):.4f} at epoch {np.argmin(self.ae_model_hist.history[\"loss\"])}')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create clusters of the input data\n",
    "def create_clusters(input_data, cluster_count = 6):\n",
    "    # create the k-means model\n",
    "    kmeans = KMeans(cluster_count, random_state = 42)\n",
    "    # fit the model\n",
    "    return kmeans.fit_predict(input_data)        \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(experiment_details, input_data, cluster_attr, experiment_results):\n",
    "    # experiment_results = pd.DataFrame(columns = ['Experiment', 'Training Loss', 'Silhouette Score'])\n",
    "    print(len(experiment_details))\n",
    "    for experiment in experiment_details:\n",
    "        print(f'Running experiment: {experiment} with layers: {experiment_details[experiment][0]} and cluster count: {experiment_details[experiment][1]}')\n",
    "        # create the autoencoder model\n",
    "        ae = auto_encoder(input_data[cluster_attr], input_data[cluster_attr], experiment_details[experiment][0])\n",
    "        # fit the model\n",
    "        ae.autoencoder_fit(epochs = 1000, batch_size = 32, verbose = 0)\n",
    "        training_loss = ae.training_loss\n",
    "        # predict the embeddings\n",
    "        ae.predict_embeddings(input_data[cluster_attr])\n",
    "        encoded_data = ae.embeddings\n",
    "        # create the clusters\n",
    "        label_str = str(experiment) + '_labels'\n",
    "        input_data[label_str] = create_clusters(encoded_data, cluster_count = experiment_details[experiment][1])\n",
    "        # calculate the silhouette score\n",
    "        silh_score_experiment = silhouette_score(input_data[cluster_attr], input_data[label_str])\n",
    "        # calculate the calinski harabasz score\n",
    "        # calinski_harabasz_score = calinski_harabasz_score(input_data, input_data[label_str])\n",
    "        # calculate the davies bouldin score\n",
    "        # davies_bouldin_score = davies_bouldin_score(input_data, input_data[label_str])\n",
    "        # Add the experiment results to the dataframe\n",
    "        experiment_results.loc[len(experiment_results)] = [experiment, training_loss, silh_score_experiment]\n",
    "    return experiment_results, input_data\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(attr_input)\n",
    "experiment_details = {'Experiment_2.1':[[input_size*2, 32, 16, 4], 4], \\\n",
    "                      'Experiment_2.2':[[input_size*2, 32, 16, 4], 6], \\\n",
    "                      'Experiment_2.3':[[input_size*2, 32, 16, 4], 8], \\\n",
    "                      'Experiment_3.1':[[input_size*2, 32, 16, 6], 4], \\\n",
    "                      'Experiment_3.2':[[input_size*2, 32, 16, 6], 6], \\\n",
    "                      'Experiment_3.3':[[input_size*2, 32, 16, 6], 8], \\\n",
    "                      'Experiment_4.1':[[input_size*2, 32, 16, 8], 4], \\\n",
    "                      'Experiment_4.2':[[input_size*2, 32, 16, 8], 6], \\\n",
    "                      'Experiment_4.3':[[input_size*2, 32, 16, 8], 8]}\n",
    "experiment_test = {'Experiment_2.1':[[input_size*2, 32, 16, 4], 4], \\\n",
    "                      'Experiment_2.2':[[input_size*2, 32, 16, 4], 6]}\n",
    "experiment_results = pd.DataFrame(columns = ['Experiment', 'Training Loss', 'Silhouette Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1.0 - Compute the results for basic input data\n",
    "kmeans_plain_cluster_count = [4, 6, 8]\n",
    "experiment_count = 0\n",
    "for cluster_count in kmeans_plain_cluster_count:\n",
    "    experiment_count += 1\n",
    "    experiment_name = 'Experiment_1.' + str(experiment_count)\n",
    "    cmd_data['kmeans_plain_labels'] = create_clusters(cmd_data[attr_input], cluster_count)\n",
    "    silh_score_experiment = silhouette_score(cmd_data[attr_input], cmd_data['kmeans_plain_labels'])\n",
    "    experiment_results.loc[len(experiment_results)] = [experiment_name, 0, silh_score_experiment]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "Running experiment: Experiment_2.1 with layers: [28, 32, 16, 4] and cluster count: 4\n",
      "Running experiment: Experiment_2.2 with layers: [28, 32, 16, 4] and cluster count: 6\n",
      "Running experiment: Experiment_2.3 with layers: [28, 32, 16, 4] and cluster count: 8\n",
      "Running experiment: Experiment_3.1 with layers: [28, 32, 16, 6] and cluster count: 4\n",
      "Running experiment: Experiment_3.2 with layers: [28, 32, 16, 6] and cluster count: 6\n",
      "Running experiment: Experiment_3.3 with layers: [28, 32, 16, 6] and cluster count: 8\n",
      "Running experiment: Experiment_4.1 with layers: [28, 32, 16, 8] and cluster count: 4\n",
      "Running experiment: Experiment_4.2 with layers: [28, 32, 16, 8] and cluster count: 6\n",
      "Running experiment: Experiment_4.3 with layers: [28, 32, 16, 8] and cluster count: 8\n"
     ]
    }
   ],
   "source": [
    "#experiment_results, cmd_data_output = run_experiments(experiment_test, cmd_data, attr_input, experiment_results)\n",
    "experiment_results, cmd_data_output = run_experiments(experiment_details, cmd_data, attr_input, experiment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Silhouette Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Experiment_1.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.283616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Experiment_1.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.323420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Experiment_1.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.332018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Experiment_2.1</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.273954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Experiment_2.2</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>0.224756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Experiment_2.3</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.274777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Experiment_3.1</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.306873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Experiment_3.2</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.292352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Experiment_3.3</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.280056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Experiment_4.1</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.248976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Experiment_4.2</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.318885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Experiment_4.3</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.245882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Experiment Training Loss  Silhouette Score\n",
       "0   Experiment_1.1             0          0.283616\n",
       "1   Experiment_1.2             0          0.323420\n",
       "2   Experiment_1.3             0          0.332018\n",
       "3   Experiment_2.1      0.001897          0.273954\n",
       "4   Experiment_2.2      0.001697          0.224756\n",
       "5   Experiment_2.3        0.0022          0.274777\n",
       "6   Experiment_3.1       0.00025          0.306873\n",
       "7   Experiment_3.2      0.000194          0.292352\n",
       "8   Experiment_3.3      0.000306          0.280056\n",
       "9   Experiment_4.1      0.000111          0.248976\n",
       "10  Experiment_4.2      0.000089          0.318885\n",
       "11  Experiment_4.3      0.000134          0.245882"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(attr_input)\n",
    "ae_layers = [input_size*2, 32, 16, 8]\n",
    "ae_model = auto_encoder(cmd_data[attr_input], cmd_data[attr_input], ae_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "# ae_model_hist = ae_model.ae_model.fit(cmd_data[attr_input], cmd_data[attr_input], epochs = 500, batch_size = 32, verbose = 1)\n",
    "ae_model.autoencoder_fit(epochs = 500, batch_size = 32, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model.print_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_data['kmeans_labels'] = create_clusters(cmd_data[attr_input], cluster_algorithm = 'k-means')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_data['hdbscan_labels'] = create_clusters(cmd_data[attr_input], cluster_algorithm = 'hdbscan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count by p_bu\n",
    "sku_count_bu = cmd_data.groupby('p_bu')['p_bu'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to plot the cluster details, getting cmd_data, cluster column name as input\n",
    "def plot_cluster_details(cmd_data, cluster_col_name, sku_count_bu):\n",
    "    ######## CLUSTER COUNT BY CLUSTER AND CLUSTER COUNT BY p_bu ########\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (20, 6))\n",
    "    # plot the cluster count\n",
    "    cluster_order = cmd_data[cluster_col_name].value_counts().sort_values(ascending = False).index\n",
    "    sns.countplot(x = cmd_data[cluster_col_name], ax = ax[0], order = cluster_order)\n",
    "    ax[0].set_title('Cluster Count')\n",
    "    ax[0].set_xlabel('Cluster')\n",
    "    ax[0].set_ylabel('Count')\n",
    "    ax[0].grid(linestyle='-', linewidth='0.5', color='red')\n",
    "    # add labels - cluster count & % of total to the bars\n",
    "    ax[0].bar_label(ax[0].containers[0], labels = \\\n",
    "                    [f'{x} ({y:.2f}%)' \\\n",
    "                     for x, y in zip(ax[0].containers[0].datavalues, \\\n",
    "                                     ax[0].containers[0].datavalues/cmd_data[cluster_col_name].value_counts().sum()*100)])\n",
    "    # plot the cluster count by p_bu\n",
    "    p_bu_order = cmd_data.groupby('p_bu')['p_bu'].value_counts().sort_values(ascending = False).index\n",
    "    hue_order = cluster_order\n",
    "    sns.countplot(hue = cmd_data[cluster_col_name], x = cmd_data['p_bu'], ax = ax[1])\n",
    "    # add labels - cluster count & % of total to the bars\n",
    "    for i in range(len(ax[1].containers)):\n",
    "        ax[1].bar_label(ax[1].containers[i], labels = \\\n",
    "                        [f'{x} ({y:.2f}%)' \\\n",
    "                         for x, y in zip(ax[1].containers[i].datavalues, \\\n",
    "                                         ax[1].containers[i].datavalues/sku_count_bu.sum()*100)])\n",
    "    ax[1].set_title('Cluster Count by p_bu')\n",
    "    ax[1].set_xlabel('Cluster')\n",
    "    ax[1].set_ylabel('Count')\n",
    "    ax[1].grid(linestyle='-', linewidth='0.5', color='red')\n",
    "    ######## PLOT OF CLUSTER COUNT WITH p_bu AND p_region AS HUE #########\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (20, 6))\n",
    "    # plot the cluster count with p_bu as hue\n",
    "    sns.countplot(hue = cmd_data['p_bu'], x = cmd_data[cluster_col_name], ax = ax[0], order = cluster_order)\n",
    "    ax[0].set_title('Cluster Count with p_bu as hue')\n",
    "    ax[0].set_xlabel('Cluster')\n",
    "    ax[0].set_ylabel('Count')\n",
    "    ax[0].grid(linestyle='-', linewidth='0.5', color='red')\n",
    "    # add labels - cluster count & % of total to the bars\n",
    "    for i in range(len(ax[0].containers)):\n",
    "        ax[0].bar_label(ax[0].containers[i], labels = \\\n",
    "                        [f'{x} ({y:.2f}%)' \\\n",
    "                         for x, y in zip(ax[0].containers[i].datavalues, \\\n",
    "                                         ax[0].containers[i].datavalues/sku_count_bu.sum()*100)])\n",
    "    # plot the cluster count with p_region as hue\n",
    "    sns.countplot(hue = cmd_data['p_region'], x = cmd_data[cluster_col_name], ax = ax[1], order = cluster_order)\n",
    "    ax[1].set_title('Cluster Count with p_region as hue')\n",
    "    ax[1].set_xlabel('Cluster')\n",
    "    ax[1].set_ylabel('Count')\n",
    "    ax[1].grid(linestyle='-', linewidth='0.5', color='red')\n",
    "    # add labels - cluster count & % of total to the bars\n",
    "    for i in range(len(ax[1].containers)):\n",
    "        ax[1].bar_label(ax[1].containers[i], labels = \\\n",
    "                        [f'{x} ({y:.2f}%)' \\\n",
    "                         for x, y in zip(ax[1].containers[i].datavalues, \\\n",
    "                                         ax[1].containers[i].datavalues/sku_count_bu.sum()*100)])\n",
    "    \n",
    "    ######## PLOT OF CLUSTER COUNT FOR p_bu = AG AND p_bu = CE #########\n",
    "    AG_cluster_order = cmd_data[cmd_data['p_bu'] == 'AG'][cluster_col_name].value_counts().sort_values(ascending = False).index\n",
    "    CE_cluster_order = cmd_data[cmd_data['p_bu'] == 'CE'][cluster_col_name].value_counts().sort_values(ascending = False).index\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (20, 6))\n",
    "    # plot the cluster count for p_bu = AG\n",
    "    sns.countplot(x = cmd_data[cmd_data['p_bu'] == 'AG'][cluster_col_name], ax = ax[0], order = AG_cluster_order)\n",
    "    ax[0].set_title('Cluster Count for p_bu = AG')\n",
    "    ax[0].set_xlabel('Cluster')\n",
    "    ax[0].set_ylabel('Count')\n",
    "    ax[0].grid(linestyle='-', linewidth='0.5', color='red')\n",
    "    # add labels - cluster count & % of total to the bars\n",
    "    ax[0].bar_label(ax[0].containers[0], labels = \\\n",
    "                    [f'{x} ({y:.2f}%)' \\\n",
    "                        for x, y in zip(ax[0].containers[0].datavalues, \\\n",
    "                                        ax[0].containers[0].datavalues/sku_count_bu['AG']*100)])\n",
    "    # plot the cluster count for p_bu = CE\n",
    "    sns.countplot(x = cmd_data[cmd_data['p_bu'] == 'CE'][cluster_col_name], ax = ax[1], order = CE_cluster_order)\n",
    "    ax[1].set_title('Cluster Count for p_bu = CE')\n",
    "    ax[1].set_xlabel('Cluster')\n",
    "    ax[1].set_ylabel('Count')\n",
    "    ax[1].grid(linestyle='-', linewidth='0.5', color='red')\n",
    "    # add labels - cluster count & % of total to the bars\n",
    "    ax[1].bar_label(ax[1].containers[0], labels = \\\n",
    "                    [f'{x} ({y:.2f}%)' \\\n",
    "                        for x, y in zip(ax[1].containers[0].datavalues, \\\n",
    "                                        ax[1].containers[0].datavalues/sku_count_bu['CE']*100)])   \n",
    "\n",
    "    ######## PLOT OF CLUSTER COUNT FOR EACH p_bu BY REGION #########\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (20, 6))\n",
    "    hue_order = cmd_data[cmd_data['p_bu'] == 'AG'][cluster_col_name].value_counts().sort_values(ascending = False).index\n",
    "    region_order = cmd_data[cmd_data['p_bu'] == 'AG']['p_region'].value_counts().sort_values(ascending = False).index\n",
    "    # plot the cluster count for p_bu = AG by region\n",
    "    sns.countplot(hue = cmd_data[cmd_data['p_bu'] == 'AG'][cluster_col_name], \\\n",
    "        x = cmd_data[cmd_data['p_bu'] == 'AG']['p_region'], ax = ax[0], hue_order = hue_order, order = region_order)\n",
    "    ax[0].set_title('Cluster Count for p_bu = AG by region')\n",
    "    ax[0].set_xlabel('Cluster')\n",
    "    ax[0].set_ylabel('Count')\n",
    "    ax[0].grid(linestyle='-', linewidth='0.5', color='red')\n",
    "    # add labels - cluster count & % of total to the bars\n",
    "    for i in range(len(ax[0].containers)):\n",
    "        ax[0].bar_label(ax[0].containers[i], labels = \\\n",
    "                        [f'{x} ({y:.2f}%)' \\\n",
    "                            for x, y in zip(ax[0].containers[i].datavalues, \\\n",
    "                                            ax[0].containers[i].datavalues/sku_count_bu['AG']*100)])\n",
    "    # plot the cluster count for p_bu = CE by region\n",
    "    hue_order = cmd_data[cmd_data['p_bu'] == 'CE'][cluster_col_name].value_counts().sort_values(ascending = False).index\n",
    "    region_order = cmd_data[cmd_data['p_bu'] == 'CE']['p_region'].value_counts().sort_values(ascending = False).index\n",
    "    sns.countplot(hue = cmd_data[cmd_data['p_bu'] == 'CE'][cluster_col_name], \\\n",
    "        x = cmd_data[cmd_data['p_bu'] == 'CE']['p_region'], ax = ax[1], hue_order = hue_order, order = region_order)\n",
    "    ax[1].set_title('Cluster Count for p_bu = CE by region')\n",
    "    ax[1].set_xlabel('Cluster')\n",
    "    ax[1].set_ylabel('Count')\n",
    "    ax[1].grid(linestyle='-', linewidth='0.5', color='red')\n",
    "    # add labels - cluster count & % of total to the bars\n",
    "    for i in range(len(ax[1].containers)):\n",
    "        ax[1].bar_label(ax[1].containers[i], labels = \\\n",
    "                        [f'{x} ({y:.2f}%)' \\\n",
    "                            for x, y in zip(ax[1].containers[i].datavalues, \\\n",
    "                                            ax[1].containers[i].datavalues/sku_count_bu['CE']*100)])   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_details(cmd_data, 'kmeans_labels', sku_count_bu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 6))\n",
    "ax = cmd_data.groupby(by = 'kmeans_labels')['p_nm'].count().sort_values(ascending = False).plot(kind = 'bar', ax = fig.add_subplot(121))\n",
    "ax.bar_label(ax.containers[0], labels = [f'{x} ({(x/sku_count)*100:,.0f}%)' for x in ax.containers[0].datavalues])\n",
    "ax.set_xlabel('Cluster')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Count of KMeans Clusters')\n",
    "ax.grid(linestyle='-', linewidth='0.5', color='red')\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax = sns.countplot(x = 'p_bu', hue = 'kmeans_labels', data = cmd_data)\n",
    "ax.set_xlabel('BU')\n",
    "ax.set_ylabel('Count by Cluster')\n",
    "ax.set_title('Count of KMeans Clusters by BU')\n",
    "ax.grid(linestyle='-', linewidth='0.5', color='red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 6))\n",
    "fig.add_subplot(121)\n",
    "# seaborn plot of the count of the clusters for p_bu = 'AG' by region\n",
    "hue_order = cmd_data[cmd_data['p_bu'] == 'AG'].groupby('kmeans_labels')['kmeans_labels'].count().sort_values(ascending = False).index\n",
    "ax = sns.countplot(x = 'kmeans_labels', data = cmd_data[cmd_data['p_bu'] == 'AG'], hue_order = hue_order)\n",
    "ax.set_xlabel('BU')\n",
    "ax.set_ylabel('Count by Cluster')\n",
    "ax.set_title('Count of KMeans Clusters for AG')\n",
    "ax.grid(linestyle='-', linewidth='0.5', color='red')\n",
    "for bar in ax.containers:\n",
    "    ax.bar_label(bar, labels = [f'{x} ({(x/sku_count_bu[\"AG\"])*100:,.0f}%)' for x in bar.datavalues])\n",
    "# plot for p_bu = 'CE'\n",
    "fig.add_subplot(122)\n",
    "hue_order = cmd_data[cmd_data['p_bu'] == 'CE'].groupby('kmeans_labels')['kmeans_labels'].count().sort_values(ascending = False).index\n",
    "ax = sns.countplot(x = 'kmeans_labels', data = cmd_data[cmd_data['p_bu'] == 'CE'], \\\n",
    "                    hue_order = hue_order)\n",
    "ax.set_xlabel('BU')\n",
    "ax.set_ylabel('Count by Cluster')\n",
    "ax.set_title('Count of KMeans Clusters for CE')\n",
    "ax.grid(linestyle='-', linewidth='0.5', color='red')\n",
    "for bar in ax.containers:\n",
    "    ax.bar_label(bar, labels = [f'{x} ({(x/sku_count_bu[\"CE\"])*100:,.0f}%)' for x in bar.datavalues])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 6))\n",
    "fig.add_subplot(121)\n",
    "# seaborn plot of the count of the clusters for p_bu = 'AG' by region\n",
    "hue_order = cmd_data[cmd_data['p_bu'] == 'AG'].groupby('kmeans_labels')['kmeans_labels'].count().sort_values(ascending = False).index\n",
    "ax = sns.countplot(x = 'p_region', hue = 'kmeans_labels', data = cmd_data[cmd_data['p_bu'] == 'AG'], \\\n",
    "                   order = cmd_data[cmd_data['p_bu'] == 'AG'].groupby('p_region')['p_region'].count().\\\n",
    "                    sort_values(ascending = False).index, hue_order = hue_order)\n",
    "ax.set_xlabel('BU')\n",
    "ax.set_ylabel('Count by Cluster')\n",
    "ax.set_title('Count of KMeans Clusters for AG')\n",
    "ax.grid(linestyle='-', linewidth='0.5', color='red')\n",
    "for bar in ax.containers:\n",
    "    ax.bar_label(bar, labels = [f'{x} ({(x/sku_count_bu[\"AG\"])*100:,.0f}%)' for x in bar.datavalues])\n",
    "# plot for p_bu = 'CE'\n",
    "fig.add_subplot(122)\n",
    "hue_order = cmd_data[cmd_data['p_bu'] == 'CE'].groupby('kmeans_labels')['kmeans_labels'].count().sort_values(ascending = False).index\n",
    "ax = sns.countplot(x = 'p_region', hue = 'kmeans_labels', data = cmd_data[cmd_data['p_bu'] == 'CE'], \\\n",
    "                   order = cmd_data[cmd_data['p_bu'] == 'CE'].groupby('p_region')['p_region'].count().\\\n",
    "                    sort_values(ascending = False).index, hue_order = hue_order)\n",
    "ax.set_xlabel('BU')\n",
    "ax.set_ylabel('Count by Cluster')\n",
    "ax.set_title('Count of KMeans Clusters for CE')\n",
    "ax.grid(linestyle='-', linewidth='0.5', color='red')\n",
    "for bar in ax.containers:\n",
    "    ax.bar_label(bar, labels = [f'{x} ({(x/sku_count_bu[\"CE\"])*100:,.0f}%)' for x in bar.datavalues])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn plot of the count of the clusters for p_bu = 'AG' by region\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 6))\n",
    "ax = cmd_data.groupby(by = 'hdbscan_labels')['p_nm'].count().sort_values(ascending = False).plot(kind = 'bar', ax = fig.add_subplot(121))\n",
    "ax.bar_label(ax.containers[0], labels = [f'{x} ({(x/sku_count)*100:,.0f}%)' for x in ax.containers[0].datavalues])\n",
    "ax.set_xlabel('Cluster')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Count of hdbscan Clusters')\n",
    "ax.grid(linestyle='-', linewidth='0.5', color='red')\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax = sns.countplot(x = 'p_bu', hue = 'hdbscan_labels', data = cmd_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the silhouette score for the k-means and hdbscan labels\n",
    "print(f'Silhouette score for k-means: {silhouette_score(cmd_data[attr_input], cmd_data[\"kmeans_labels\"]):.4f}')\n",
    "print(f'Silhouette score for hdbscan: {silhouette_score(cmd_data[attr_input], cmd_data[\"hdbscan_labels\"]):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the silhouette score for each cluster using mean of silhouette_samples for each cluster\n",
    "# get the silhouette score for each sample\n",
    "sample_silhouette_values = silhouette_samples(cmd_data[attr_input], cmd_data['kmeans_labels'])\n",
    "silh_score_per_cluster = []\n",
    "for i in range(8):\n",
    "    silh_score_per_cluster.append(sample_silhouette_values[cmd_data['kmeans_labels'] == i].mean()) \n",
    "  \n",
    "silh_score_per_cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = cmd_data[attr_input].shape[1]\n",
    "l1_size = input_size*2\n",
    "l2_size = 32\n",
    "l3_size = 16\n",
    "bottleneck_size = 8\n",
    "encoder_input = Input(shape = (input_size, ))\n",
    "# encoder layer 1\n",
    "encoder_m5_l1 = Dense(l1_size)(encoder_input)\n",
    "encoder_m5_l1 = LeakyReLU()(encoder_m5_l1)\n",
    "# encoder layer 2\n",
    "encoder_m5_l2 = Dense(l2_size)(encoder_m5_l1)\n",
    "encoder_m5_l2 = LeakyReLU()(encoder_m5_l2)\n",
    "# encoder layer 3\n",
    "encoder_m5_l3 = Dense(l3_size)(encoder_m5_l2)\n",
    "encoder_m5_l3 = LeakyReLU()(encoder_m5_l3)\n",
    "# encoder bottleneck layer\n",
    "encoder_m5_output = Dense(bottleneck_size)(encoder_m5_l3)\n",
    "encoder_m5_output = LeakyReLU()(encoder_m5_output)\n",
    "# decoder layer 1\n",
    "decoder_m5_l1 = Dense(l3_size)(encoder_m5_output)\n",
    "decoder_m5_l1 = LeakyReLU()(decoder_m5_l1)\n",
    "# decoder layer 2\n",
    "decoder_m5_l2 = Dense(l2_size)(decoder_m5_l1)\n",
    "decoder_m5_l2 = LeakyReLU()(decoder_m5_l2)\n",
    "# decoder layer 3\n",
    "decoder_m5_l3 = Dense(l1_size)(decoder_m5_l2)\n",
    "decoder_m5_l3 = LeakyReLU()(decoder_m5_l3)\n",
    "# decoder output layer\n",
    "decoder_m5_output = Dense(input_size, activation = 'linear')(decoder_m5_l3)\n",
    "# define the autoencoder model\n",
    "ae_fixed_m5 = Model(encoder_input, decoder_m5_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_fixed_m5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "ae_fixed_m5.compile(optimizer='adam', loss='mse')\n",
    "# fit the model\n",
    "ae_fixed_m5_hist = ae_fixed_m5.fit(cmd_data[attr_input], cmd_data[attr_input], epochs = 500, batch_size = 32, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the loss at epochs 100, 500, 999 and min loss. Format loss to 4 decimal places using f strings\n",
    "print(f'Loss at epoch 100: {ae_fixed_m5_hist.history[\"loss\"][99]:.4f}')\n",
    "print(f'Loss at epoch 500: {ae_fixed_m5_hist.history[\"loss\"][499]:.4f}')\n",
    "#print(f'Loss at epoch 999: {ae_fixed_m5_hist.history[\"loss\"][998]:.4f}')\n",
    "print(f'Min loss: {min(ae_fixed_m5_hist.history[\"loss\"]):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
